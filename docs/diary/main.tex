\documentclass{article}
\setcounter{secnumdepth}{0}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{color}
\usepackage{caption}
\usepackage{enumitem}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{hyperref, todonotes}


\title{Environment mapping with a mobile robot \\
\Large MSc thesis diary}
\author{Laszlo Debreczeni}
\date{March 2024 -- December 2024}
\graphicspath{ {./images/} }

\begin{document}
\maketitle

\tableofcontents

\newpage

\section{Oct 22 -- Oct 29}
\subsection{Tasks}
\begin{itemize}
    \item Create a node that can save required data from the SPAI node to create GSplats/train NeRFs. \todo[color=green!30]{DONE}
    \item Continue writing the thesis \todo[color=blue!30]{IN PROGRESS}
\end{itemize}

\subsection{Achievements}
\begin{itemize}
    \item I have created a new package under the laptop's workspace. It contains a node which subscribes onto the \verb|/slam/keyframe| and \verb|/slam/left| topics. The node saves the latest image from the latter topic for it can be a potential keyframe. Messages are sent to the \verb|/slam/keyframe| topic only when the camera sees a new keyframe, so when this happens, the node saves the latest image into a \verb|.png| file because it is a keyframe.
    \item The node does not do anything else because I wanted it to be as fast as possible. After the images are saved there could be blurry or overexposed ones among them so we have to remove them and renumber the images in the folder. The \verb|post_process_images.py| script does that: the blurriness can be measured by the Laplacian of the image (the higher the value, the more sharp the image is) and the exposure by the mean brightness of the image (higher value means more brightness). If the Laplacian is lesser than the given threshold (500 by default because it worked fine) or the exposure is greater than the given threshold (200 by default because it worked fine) then the image is deleted. After the deletions of the images the remaining ones are renumbered sequentially in the folder.
    \item We have to generate a COLMAP from these pictures in order to create a Gaussian Splat or train a NeRF. We can do it with nerfstudio with the following command: ns-process-data images --data PATH/TO/keyframes --output-dir PATH/TO/OUTPUT. It may take a few minutes to run if we have a lot of saved keyframes.
    \item With the generated COLMAP we are finally able to create our Gaussian Splat. It can be done by the nerfstudio's ns-train splatfacto --data gsplat\_input/ or by the ns-train splatfacto-big --data gsplat\_input command. I used the big one because it can create more precise splats. Then we can run the following to export our splat into a \verb|.ply| file: ns-export gaussian-splat --load-config outputs/gsplat\_input/splatfacto/DATE/config.yml --output-dir exports/splat/.
    \item Fortunately nerfstudio has been updated and I could create my own splats with 6 GB of VRAM now, a while ago it was not possible.
    \item Unfortunately we can only save monochrome pictures because the SPAI camera pipeline does not open the RGB camera and we cannot open it from outside the pipeline because it is "locked".
    \item Created splat:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/spai_gsplat1.png}
        \captionof{figure}{Created GSplat}
    \end{minipage}
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/spai_gsplat2.png}
        \captionof{figure}{Created GSplat}
    \end{minipage}
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/spai_gsplat3.png}
        \captionof{figure}{Created GSplat}
    \end{minipage}
    
\end{itemize}

\newpage

\section{Oct 15 -- Oct 22}
\subsection{Tasks}
\begin{itemize}
    \item Make the SPAI node work. \todo[color=green!30]{DONE}
    \item Separate the SPAI node into 2 ROS2 workspaces: one for the Turtlebot4 and one for the laptop.
\end{itemize}

\subsection{Achievements}
\begin{itemize}
    \item I have been on holiday between Oct 14 -- Oct 18.
    \item The SPAI node works. Some weeks ago we tried to use it with an old SDK version that Gabor got from the SPAI Team but it was not compatible with our code so we removed some segments from it. One segment was the registration of our callback function from the camera pipeline. I put it back and now it works well, the position, orientation and pointcloud from VIO is published well.
    \item I have separated the code into 2 workspaces. The mapping and the visualization now has different ROS2 workspaces so we do not have to build every package on the Turtlebot or on the laptop.
\end{itemize}

\newpage

\section{Oct 8 -- Oct 15}
\subsection{Tasks}
\begin{itemize}
    \item Make the SPAI node work. \todo[color=green!30]{DONE}
\end{itemize}

\subsection{Achievements}
\begin{itemize}
    \item The node did not work because the callback functions did not get called. I have asked the SPAI team but they have not answered yet.
\end{itemize}

\newpage

\section{Sep 24 -- Oct 8}
\subsection{Tasks}
\begin{itemize}
    \item Improve the spectacularAI mapping launch scripts. \todo[color=green!30]{DONE}
    \item Make the thesis work on my new Zenbook \todo[color=green!30]{DONE}
\end{itemize}

\subsection{Achievements}
\begin{itemize}
    \item I have improved the mapping launch script which runs on the Turtlebot. Now we can do the following:
    \begin{itemize}
        \item Do SLAM
        \item Record the movement for GSPlat/NeRF training
        \item Do SLAM and record simultaneously
    \end{itemize}
    \item Unfortunately my Zenbook cannot boot from the external HDD which has Ubuntu and every dependency installed. I tried WSL but the camera could not be used because the camera appears as different USB devices during startup and the actual functioning. Due to this I could not bind it correctly to WSL. The solution was VirtualBox with a fully functioning Ubuntu installed on it, my custom launch scripts successfully started on it.
\end{itemize}

\newpage

\section{Sep 17 -- Sep 24}

\subsection{Tasks}
\begin{itemize}
    \item Ask Spectacular AI team if they are willing to send us their SDK for \verb|aarch64| architecture for my thesis (it is commercial). \todo[color=green!30]{DONE}
    \item Try out \verb|nvblox|. \todo[color=green!30]{DONE}
    \item Try out \verb|isaac-vslam|. \todo[color=green!30]{DONE}
    \item Try out \url{https://github.com/msardonini/oakd_isaac_ros/tree/main?tab=readme-ov-file} \todo[color=green!30]{DONE}
\end{itemize}

\subsection{Achievements}
\begin{itemize}
    \item Spectacular AI has sent me their SDK for \verb|aarch64| architecture. This is a commercial version but I was able to negotiate with them using my university student status and thesis writing. The next time we meet at Nokia we should be able to try it out.
    \item I have tried out \verb|nvblox|, sometimes the example works well but most of the time it crashes. This is due to the requirement of 8 GB VRAM, my laptop only has 6 (it has a GTX1660 Ti). My PC has an RTX 3070 Ti with 8 GB VRAM so it should be enough but I was not able to boot it up from the external HDD. I tried every setup option related to booting and USB but it did not work. The PC sees the HDD as a bootable device but when I try to boot from it only a text appears that says it did not find any bootable media, I should insert one and try again. I even tried all the USB ports on the computer but none of them worked. Nvblox when it started:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/nvblox.png}
        \captionof{figure}{nvblox example}
    \end{minipage}
    \item I have tried out the \verb|nvidia isaac vslam| example:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/isaac_vslam_example.png}
        \captionof{figure}{isaac vslam example}
    \end{minipage}
    \item I experimented with \verb|isaac vslam| using Nvidia's \verb|isaac sim| omniverse tool. It did not work well because the simulator needs an RTX GPU with at least 8 GB VRAM. I tried this example: \url{https://nvidia-isaac-ros.github.io/concepts/visual_slam/cuvslam/tutorial_isaac_sim.html} hoping that the simulator could work with my graphics card (GTX1660 Ti) but nothing appeared in RViz so my GPU is not compatible with this simulator.
    \item I experimented with \url{https://github.com/msardonini/oakd_isaac_ros/tree/main?tab=readme-ov-file}. It could not build the Docker image because there are attribute errors. I tried to build it with newer versions of isaac ros but they were not compatible with each other.
\end{itemize}

\section{Sep 10 -- Sep 17}
\subsection{Ideas with SpectacularAI}
High priority: SpectacularAI VIO and VPS combination on the Turtlebot4.
\begin{itemize}
\item get SpectacularAI with OAKD working again. Combine SPAI VIO with Peter's VPS client node and collect local+global pose pairs. Calculate alignment transform between local map and global map.
\item the goal is that we can start the robot anywhere on the floor, it will track its own local motion using SPAI, and via VPS we can localize in the building coordinate system, and align the local coordinate system to the global coordinate system
\end{itemize}

\subsection{Ideas to work with ORB-SLAM:}
\begin{itemize}
    
\item generate lidarmap based on OS2 map, like in this project:
\item \url{https://github.com/abhineet123/ORB_SLAM2/blob/master/2d-grid-mapping.pdf}
\item \url{https://github.com/abhineet123/ORB_SLAM2/blob/master/Examples/ROS/ORB_SLAM2/src/ros_mono_sub.cc}
\item \url{https://github.com/abhineet123/ORB_SLAM2/blob/master/pointCloudToGridMap2D.py}

\item Can we do the same not from OS2map, but from a Colmap model? As you know, I can export OS3 map and our own SLAM's map into Colmap format, which has all the necessary information. Then we could use an RGBD camera to map the space, create a 3D reconstruction with any method, export it to Colmap format, and then turn that into a navigable 2D lidarmap. That would be cool.

\item ExplORBSLAM \url{https://github.com/JulioPlaced/ExplORB-SLAM} (Laci: this works on ROS1)

\item Laci: I think we may be able to create a 2D lidar map from the Colmap because it contains the recorded point cloud. We just have to delete the points that represent the ceiling, ignore the points that represent the ground and represent the obstacled detected by the lidar from those points that are in the robot's way. This could work.

\end{itemize}

\subsection{Ideas to work with RTAB-Map}
\begin{itemize}
\item \url{https://introlab.github.io/rtabmap/}
\item RTAB-Map navigation for TB3. Let's rewrite this for TB4! \url{
https://github.com/introlab/rtabmap_ros/blob/master/rtabmap_demos/launch/demo_turtlebot3_navigation.launch} (Laci: there is a slam example on the \verb|ros2| branch for Turtlebot4: \url{https://github.com/introlab/rtabmap_ros/blob/ros2/rtabmap_demos/launch/turtlebot4_slam.launch.py}). Edit Laci: I tried it out but it froze everytime, sometimes the robot could not even undock in Gazebo and if it did, I tried to teleop it but it froze after some seconds. I tried with the mentioned explorer but it just waits for a costmap which is not present. Even RViz did not work well, when I sent a goal the robot just stood in one place.

\item \url{https://sudonull.com/post/18423-Localization-and-navigation-in-ROS-using-rtabmap} (Laci: this is for ROS1)

\item RTAB-Map drone navigation \url{https://github.com/matlabbe/rtabmap_drone_example}. It seems they only do 2D planning. How do they genreate the costmap from the RTABMAP? Think about how we could do 3D (also up-down) planning. (Laci: the costmap is generated here \url{https://github.com/matlabbe/rtabmap_drone_example/blob/3055aea3a69da6c8acc0177c4c9c386578df38d5/launch/slam.launch#L39C5-L48C13} and some layer plugins are used as well \url{https://github.com/matlabbe/rtabmap_drone_example/blob/3055aea3a69da6c8acc0177c4c9c386578df38d5/param/local_costmap_params.yaml#L18C3-L20C65}. I went through the whole repo but could not find anything else.)

\item object detection (2D to 3D) demo: \url{https://github.com/introlab/rtabmap_ros/blob/master/rtabmap_demos/launch/demo_find_object.launch}
\item WiFi signal strength visualization demo: \url{https://github.com/introlab/rtabmap_ros/blob/master/rtabmap_demos/src/WifiSignalSubNode.cpp} and \url{https://github.com/introlab/rtabmap_ros/blob/master/rtabmap_demos/scripts/wifi_signal_pub.py}
\item multi-session mapping (as described on their main page at June 2022): \url{https://github.com/introlab/rtabmap/tree/master/archive/2022-IlluminationInvariant}
\end{itemize}


\subsection{Ideas to  work with nvblox}
Test the requirements and decied whether feasible on the TB4
\begin{itemize}
    \item nvblox supports RealSense cameras (there is one on the Turtlebot4)
    \item \url{https://forums.developer.nvidia.com/t/nvblox-nav2-implementation/272598/2}
    \item \url{https://forums.developer.nvidia.com/t/how-to-use-nvblox-with-luxonis-oak-d-pro-poe/282877}
\end{itemize}


\newpage

\section{Sep 3 -- Sep 10}
\begin{itemize}
\item new literature review (Gabor put into GDrive) \todo[color=green!30]{DONE}
\item review last semester's code and make sure it all works. \todo[color=green!30]{DONE}
\end{itemize}

\subsection{Achievements}
\begin{itemize}
    \item The code I wrote last semester to detect persons and publish their positions to a ROS2 topic still works. On the other hand it still has the bug in the DepthAI SDK: after successful detections a few seconds later the camera freezes telling that the IMU buffer size is exceeded. It happens with the default DepthAI script too, they have not fixed it...
\end{itemize}

------------------------------------------------




\section{May 28 -- June 4}
\begin{itemize}
\item presentation \todo[color=green!30]{DONE}
\end{itemize}

\section{May 21 -- May 28}
\begin{itemize}
\item report writing, finalization \todo[color=green!30]{DONE}
\end{itemize}

\section{May 14 -- May 21}
\begin{itemize}
\item testing 3D person detection on Turtlebot4 \todo[color=green!30]{DONE}
\item report writing \todo[color=green!30]{DONE}
\end{itemize}


\section{May 07 -- May 14}
\begin{itemize}
\item testing SPAI pose estimation on Turtlebot4 \todo[color=green!30]{DONE}
\item testing 3D person detection on Turtlebot4 \todo[color=green!30]{DONE}
\end{itemize}
\newpage

\section{April 30 -- May 07}
\subsection{Reading}
Some links on neural 3D maps:
\begin{itemize}
\item NerfNavigation: Vision-Only Robot Navigation in a Neural Radiance World, \url{https://mikh3x4.github.io/nerf-navigation/} \todo[color=green!30]{DONE}
\item Motion Planning with Implicit Neural Representations of Geometry, \url{https://neural-implicit-workshop.stanford.edu/} \todo[color=green!30]{DONE}
\item Splat-Nav: Safe Real-Time Robot Navigation in Gaussian Splatting Maps, \url{https://arxiv.org/abs/2403.02751} \todo[color=green!30]{DONE}
\item SplaTAM: Splat, Track \& Map 3D Gaussians for Dense RGB-D SLAM\url{https://spla-tam.github.io/} \todo[color=green!30]{DONE}
\item \url{https://github.com/zubair-irshad/Awesome-Implicit-NeRF-Robotics?tab=readme-ov-file#planningnavigation}
\end{itemize}
\newpage

\section{April 23 -- April 30}

\subsection{Reading}
Literature search about 3D map representations for robot/drone navigation: point cloud map, voxel map, triangle mesh map, 3DGS map, topological map. Which one is the best for 3D camera-based navigation? Which ones can we create with the Turtlebot's sensors?

Some links on traditional 3D maps:
\begin{itemize}
\item Octomap: \url{https://octomap.github.io/} \todo[color=green!30]{DONE}
\item Voxblox: Oleynikova - Voxblox: Incremental 3D Euclidean Signed Distance Fields for On-Board MAV Planning \url{https://helenol.github.io/publications/iros_2017_voxblox.pdf} \todo[color=green!30]{DONE}
\item VoxboxPlanning: \url{https://github.com/ethz-asl/mav_voxblox_planning} \todo[color=green!30]{DONE}
\item nvBlox \url{https://nvblox.readthedocs.io/en/latest/} -- top priority \todo[color=green!30]{DONE}
\item MeshNavigation \url{https://github.com/naturerobots/mesh_navigation} \todo[color=green!30]{DONE}
\item RTAB-Map: \url{https://introlab.github.io/rtabmap/} \todo[color=green!30]{DONE}
\item maplab, maplab 2.0 \url{https://maplab.asl.ethz.ch/docs/master/index.html} \todo[color=green!30]{DONE}
\item FUEL: Boyu Zhou, Yichen Zhang, Xinyi Chen, and Shaojie Shen. Fuel: Fast \todo[color=green!30]{DONE}
uav exploration using incremental frontier structure and hierarchical
planning. IEEE Robotics and Automation Letters, 6(2):779â€“786, 2021.
\item [2024] Zeng - Autonomous Implicit Indoor Scene Reconstruction with Frontier Exploration (arxiv) \todo[color=green!30]{DONE}
\end{itemize}
\newpage

\section{April 16 -- April 23}

\subsection{Tasks}
\begin{itemize}
\item person detector + estimate 3D position of person \todo[color=green!30]{DONE}
\item try DepthAI RTAB-Map ROS \url{https://github.com/luxonis/depthai-ros/blob/humble/depthai_ros_driver/launch/rtabmap.launch.py} \todo[color=green!30]{DONE}
\item Optional: try RTAB-MAP iOS \url{https://apps.apple.com/ca/app/rtab-map-3d-lidar-scanner/id1564774365} \todo[color=green!30]{DONE}
\item try to load and continue a SPAI map \todo[color=blue!30]{IN PROGRESS}
\end{itemize}

\subsection{Achievements}
\begin{itemize}
    \item Person position estimation: with the help of the luxonis sdk-examples, we can estimate a person's position with the \verb|depthai_combination.py| script. X: horizontal position, Y: vertical position, Z: distance from the camera. It uses the camera's frame but we can easily calculate the world coordinates with the help of \url{https://docs.luxonis.com/projects/api/en/latest/components/nodes/spatial_location_calculator/}
    \item DepthAI ros \verb|ros2 launch depthai_ros_driver camera.launch.py| with RViz2:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/depthai_ros_camera.png}
        \captionof{figure}{Camera's image in RViz2}
    \end{minipage}
    \item When I tried running the \verb|ros2 launch depthai_ros_driver rtabmap.launch.py| command it did not found the \url{https://github.com/introlab/rtabmap_ros/tree/ros2#rtabmap_ros} package so I installed it. After this it was running fine but due to midnight I could not test it thoroughly.\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/rtabmap_ros.png}
        \captionof{figure}{RTabmap in action}
    \end{minipage}\par
    I tried testing it afternoon. Sometimes it does not want to load the 3D map section at all. When I build a map it always pauses itself because it wants to detect loop closures. It is really hard to go back to the position it wants to close the loop with especially in darker places. I think it will be quite difficult to use it on the real robot if it is going to produce these problems.

    \item When I tried to replay a bigger mapping with SPAI's \verb|mapping_visu.py| it could not reach the end of it because its buffer only accepts a maximum of 100 images.
    \item Loading and continuing a SPAI map will be quite tricky because the mapping examples are just able to record a mapping or replay a previously recorded one. I could not find a way to continue a recorded map. It will be possible maybe by tinkering with the sdk (we have to write a custom script) or with these: \url{https://docs.luxonis.com/en/latest/pages/slam_oak/}, \url{https://iopscience.iop.org/article/10.1088/1742-6596/2467/1/012029/pdf}
    \item RTabmap iOS:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/rtabmap_ios.png}
        \captionof{figure}{RTabmap iOS generated mesh}
    \end{minipage}\par

    \item We tried out RTabmap with Gabor in the office and we saved the point cloud map:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/point_cloud_nokia_office.png}
        \captionof{figure}{RTabmap generated point cloud at Nokia}
    \end{minipage}\par

    \item This publishes detections: \url{https://github.com/tiiuae/depthai_ctrl}
    
\end{itemize}
\newpage

\section{April 9 -- April 16}
\subsection{Reading}
\begin{itemize}
\item 3D Gaussian Splatting, \url{https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/}
\end{itemize}

\subsection{Tasks}
\begin{itemize}
\item SPAI point cloud map saving and loading, SPAI localization \todo[color=green!30]{DONE}
\item SPAI keyframe+pose saving \todo[color=green!30]{DONE}
\item SPAI mesh saving (optional, if possible) \todo[color=green!30]{DONE}
\item convert SPAI dataset to NerfStudio dataset, reconstruct your room with NerfStudio's gsplat implementation \todo[color=green!30]{DONE}
\item Can you write a script that converts a SPAI dataset into an input dataset for the original 3DGS?\url{https://github.com/graphdeco-inria/gaussian-splatting} Alternatively, can you create a script that converts a NerfStudio dataset to an input dataset for the original 3DGS?\todo[color=green!30]{DONE}
\item try a 3D Gaussian Splatting reconstruction of your room (requires GPU) \todo[color=green!30]{DONE}
\item try SuperSplat 3DGS editor (for your LumaAI model) \url{https://playcanvas.com/supersplat/editor} \todo[color=green!30]{DONE}
\item try Web-based 3DGS viewer \url{https://github.com/mkkellogg/GaussianSplats3D} \todo[color=green!30]{DONE}
\end{itemize}

\subsection{Achievements}
\begin{itemize}
    \item SPAI point cloud map can be saved by the mapping example. We can export optimized point clouds in ply and pcd formats with \verb|sai-cli|
    \item I could not figure out how could we load and visualize the saved point cloud map yet.
    \item The keyframes and poses can be extracted from the mapping with the help of \verb|sai-cli process|. It saves the keyframes as images and the corresponding 3D points.
    \item The mesh cannot be saved by the pure SDK examples. A video can be used to create the mesh on it and a map can be loaded for the example.
    \item When trying to use the nerfstudio's splatfacto model (Gaussian Splatting), the dromni's docker image throws a CUDA error. The nerfstudio's official image throws this error:\par
    assert block\_width \verb|>| 1 and block\_width \verb|<|= 16, "block\_width must be between 2 and 16"
    AssertionError: block\_width must be between 2 and 16\par
    It is a problem with nerfstudio and/or gsplat. I tried installing older gsplat versions but the problem remained.
    \item I tried the Nerfstudio's splatfacto model once more and now it started training. The only issue with this is the following: I have a GTX1660 in my laptop with 6GB of VRAM and it is not enough for it. I always get a \verb|CUDA out of memory| error, because the model is not optimized for lower VRAMs.
    \item SPAI to 3DGS: the output of the \verb|sai-cli| command are a COLMAP and the keyframe images, this can be used by the 3DGS.
    \item I tried the original GSplat \url{https://github.com/graphdeco-inria/gaussian-splatting}. It worked for me except the \verb|CUDA out of memory error| Gabor: point clouds can be opened and edited by MeshLab or CloudCompare
    \item I tried using Google Colab for training the Gaussian Splatting:
    \begin{itemize}
        \item It worked for 1000, 2000, 5000 iterations with 150 images so far.
        \item It does not work for 7000 or more iterations with 150 images.
    \end{itemize}
\item I have tried SuperSplat but I could not export the .ply file from my LumaAI model (it only supports its own web browser, I could not export any files from it, only a link that opens the LumaAI's online viewer)
\item I did not install the mkkellogg's web based 3DGS viewer, there is a webpage where you can try it out without installing. The original \verb|gaussian-splatting|'s \verb|SIBR Viewer| was faster, more robust and it was easier to use.
\end{itemize}


\newpage


\section{March 28 -- April 9}
\subsection{Reading}
\begin{itemize}
    \item read about neural radiance fields \url{https://www.youtube.com/@thenerfguru}
    \item read about 3D Gaussian Splatting \url{https://www.youtube.com/watch?v=VkIJbpdTujE}
\end{itemize}

\subsection{Tasks}
\begin{itemize}
\item try LumaAI \url{https://lumalabs.ai/} with iPhone RGBD \todo[color=green!30]{DONE}
\item convert SPAI dataset to NerfStudio dataset (SPAI script does that) \todo[color=green!30]{DONE}
\item optionally try a NeRFStudio reconstruction of your room (requires GPU) \todo[color=green!30]{DONE}
\end{itemize}

\subsection{Achievements}
\begin{itemize}
    \item LumaAI: \url{https://lumalabs.ai/capture/00CC75E0-E9C1-42C2-8530-C8D8995CF6A9}
    \item SPAI dataset to NeRFStudio dataset: \verb|sai-cli| command can do that
    \item For the room reconstruction see the previous week's achievements
\end{itemize}

\newpage



\section{March 21 -- March 28}
\subsection{Tasks}
\begin{itemize}
\item Create own ROS2 workspace and create an own ROS2 node \url{https://docs.ros.org/en/foxy/Tutorials/Beginner-Client-Libraries/Creating-A-Workspace/Creating-A-Workspace.html} \todo[color=green!30]{DONE}
\item test SpectacularAI SDK \url{https://github.com/SpectacularAI/sdk-examples} \todo[color=green!30]{DONE}
\item VIO record, replay, visu \url{https://github.com/SpectacularAI/sdk-examples/tree/main/python/oak} \todo[color=green!30]{DONE}
\item ROS wrapper: \url{https://github.com/SpectacularAI/sdk-examples/tree/main/python/oak/ros2} \todo[color=green!30]{DONE}
\item Mapping ROS \url{https://github.com/SpectacularAI/sdk-examples/blob/main/python/oak/mapping_ros.py}: This works only with ROS1!
\item create a point cloud reconstruction of your room
\url{https://www.youtube.com/watch?v=n34dt-Ag1Yo} \todo[color=green!30]{DONE}
\end{itemize}

\subsection{Achievements}
\begin{itemize}
    \item Unfortunately, due to HDD crash and reinstall, the project is delayed by one week. The same tasks are valid for the next week. The solution was to buy a high speed, more reliable external HDD (ADATA HD650 in my case)
    \item A basic ROS2 workspace with packages containing nodes has been created \url{https://github.com/K0pasz/thesis_ros2_test_workspace}
    \item When I try out the SpectecularAI Python examples they log onto the terminal "SpectacularAI WARN: Dropping frames!
SpectacularAI WARN:   VIO may be running too slow, data is being input too fast, or IMU samples are missing / time-offset from frames. (buffer size 10)". SOLUTION: The problem was that the IMU's firmware version was too old. Had to update it, but the documentation is extremely poor about it. What I did: cloned \url{https://github.com/luxonis/depthai-python}, then ran \verb|python3 examples/install_requirements.py| and \verb|python3 examples/IMU/imu_firmware_update.py| which successfully updated the IMU's firmware.
    \item After the successful firmware update, I tried different Spectacular AI examples.
    \item IMU visualization:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{spectacular_ai_vio_visu.png}
        \captionof{figure}{IMU visualization}
    \end{minipage}
    \item 3D Pen: if you cover the camera with your hand, you can draw in the air. Video: videos/spectacular\_ai\_3d\_pen.webm
    \item If you want to make the \verb|mapping_visu.py| work, you have to comment out a several lines inside \verb|OpenGL/contexdata.py|:
    \begin{verbatim}
        def getContext( context = None ):
    """Get the context (if passed, just return)
    
    context -- the context ID, if None, the current context
    """
    if context is None:
        context = platform.GetCurrentContext()
        # if context == 0:
        #     from OpenGL import errorS
        #     raise error.Error(
        #         """Attempt to retrieve context when no valid context"""
        #     )
    return context
    \end{verbatim}
    After this, it should work like a charm:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{spectacular_ai_mapping_visu.png}
        \captionof{figure}{Mapping visualization}
    \end{minipage}
    \item Mapping AR: it can create a mesh and point cloud representation of the surroundings:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{spectacular_ai_mapping_ar_mesh1.png}
        \captionof{figure}{Mapping AR with mesh}
    \end{minipage}
    \par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{spectacular_ai_mapping_ar_mesh2.png}
        \captionof{figure}{Mapping AR with mesh}
    \end{minipage}
    \par
    The point cloud representation was more informative:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{spectacular_ai_mapping_ar_pc1.png}
        \captionof{figure}{Mapping AR with point cloud}
    \end{minipage}
    \par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{spectacular_ai_mapping_ar_pc2.png}
        \captionof{figure}{Mapping AR with point cloud}
    \end{minipage}
    \par
    \item There is an example where ros2 is used for mapping and you can view the created point cloud in RViz:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{spectacular_ai_mapping_ros2.png}
        \captionof{figure}{Mapping with ROS2 and RViz}
    \end{minipage}\par
    You can see the chairs, table and fridge clearly, this works awesome!
    \item One of the most spectacular example is the one that uses depthai. It can detect objects and place them in space with absolute positions:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/spectacular_ai_depthai_combination.png}
        \captionof{figure}{Object detection and position estimation with DepthAI}
    \end{minipage}\par
    You can modify the source to detect specific objects. I modified the axes' bounds too so the objects' position is inside the bounding box.
    \item The mixed reality example places a cube in front of the camera's initial position and it stays there (though it shakes a bit sometimes) so you can move the camera around it:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/spectacular_ai_mixed_reality.png}
        \captionof{figure}{Mixed reality cube}
    \end{minipage}\par
    \item According to the documentation, AprilTags can be used too.
    \item Point cloud reconstruction of my room:
    \begin{itemize}
        \item I used the SpectacularAI's \verb|mapping_visu.py| script to create videos with the camera (\verb|python3 mapping_visu.py --recordingFolder /PATH/TO/FOLDER|)
        \item The \verb|sai-cli process| command creates the keyframes for the NeRF \verb|sai-cli process --format FORMAT --preview --preview3d INPUT OUTPUT|\par
        \begin{minipage}{\linewidth}
            \centering
            \includegraphics[width=1\linewidth]{images/sai-cli_process.png}
            \captionof{figure}{sai-cli command for creating input for NeRF}
        \end{minipage}\par
        \item Then I used nerfstudio in Docker to create the scene:\par
        \begin{verbatim}
            sudo docker run --gpus all \
            -u $(id -u) \
            -v /home/laci/mappings/:/workspace/ \
            -v /home/laci/.cache/:/home/user/.cache \
            -p 7007:7007 --rm -it \
            --shm-size=1gb dromni/nerfstudio:1.0.3
        \end{verbatim}
        \item You can train the NeRF with the \verb|ns-train nerfacto --data PATH_TO_DATA| command
        \item You need to have the nVidia driver installed to be able to do this. \par
        \begin{minipage}{\linewidth}
            \centering
            \includegraphics[width=1\linewidth]{images/nerfstudio.png}
            \captionof{figure}{nerfstudio during training}
        \end{minipage}\par
        \item After the training has completed you can view the trained model with:\par
        \begin{verbatim}
            ns-viewer --load-config /workspace/outputs/done/nerfacto/2024-04-05_202416/config.yml
        \end{verbatim}\par
        \begin{minipage}{\linewidth}
            \centering
            \includegraphics[width=1\linewidth]{images/trained_nerf_karcag1.png}
            \captionof{figure}{Trained NeRF in our living room in Karcag}
        \end{minipage}\par
        \item At some poses the rendering was not so successful because I did not took enough time to shoot the room thoroughly with the camera.
        
    \end{itemize}

    \item It would be interesting to try 3DGS and compare it with nerfstudio: \url{https://github.com/graphdeco-inria/gaussian-splatting}
    
\end{itemize}

\newpage

\section{March 12 -- March 19}

\subsection{What did I do}
\begin{itemize}
    \item DepthAI viewer:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{depthai_viewer.png}
        \captionof{figure}{DepthAI Viewer}
    \end{minipage}
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{depthai_viewer_3d.png}
        \captionof{figure}{DepthAI Viewer 3D reconstruction}
    \end{minipage}
    \item \url{https://docs.luxonis.com/en/latest/pages/slam_oak/}
    \item \url{https://github.com/SpectacularAI/sdk-examples/tree/main/python/oak}
        
\end{itemize}

\subsection{Reading}
Read about 
\begin{itemize}
    \item ORB\_SLAM (\url{https://github.com/raulmur/ORB_SLAM}) \todo[color=green!30]{DONE}
    \item ORB\_SLAM2 (\url{https://github.com/raulmur/ORB_SLAM2})\todo[color=green!30]{DONE}
    \item ORB\_SLAM3 (\url{https://github.com/UZ-SLAMLab/ORB_SLAM3}) \todo[color=green!30]{DONE}
    \item \url{https://webdiis.unizar.es/~raulmur/orbslam/} \todo[color=green!30]{DONE}
\end{itemize}


example citation \cite{Macenski2021}


\subsection{Links}
\begin{itemize}
\item ORB\_SLAM3 with OAKD Pro \url{https://github.com/duncanrhamill/oakd_orbslam3}
\end{itemize}

\subsection{Tasks}
\begin{itemize}
    \item Install the OAK-D camera's dependencies \todo[color=green!30]{DONE}
    \item Try out the DepthAI demos with the OAK-D camera \todo[color=green!30]{DONE}
\end{itemize}

\newpage

\section{March 5 -- March 12}
\begin{itemize}
    \item Turtlebot4 Lite offers maximum current of 1.9 A but Turtlebot4 (the more advanced one) offers only 300 mA \textrightarrow it's kind of weird
    \item Maybe static IP or self-evident hostname for the RPi?
    \item SpectecularAI could be used for large-scale mapping and real-time reconstruction of the area that is mapped with the camera
    \item This could be useful: \url{https://docs.luxonis.com/en/latest/pages/spatial-ai/}
    \item Using custom models on the OAKD: \url{https://docs.luxonis.com/en/latest/pages/model_conversion/}
    \item Running a model on low performance devices (such as the RPi on the Turtlebot4): \url{https://docs.luxonis.com/en/latest/pages/tutorials/local_convert_openvino/}
    \item Camera depth map to point cloud: \url{https://docs.luxonis.com/en/latest/pages/tutorials/device-pointcloud/}
\end{itemize}

\subsection{Reading}
\begin{itemize}
\item read about Turtlebot4 \url{https://turtlebot.github.io/turtlebot4-user-manual/}\todo[color=green!30]{DONE}

\item read about ROSBot2 \url{https://husarion.com/manuals/rosbot/}\todo[color=green!30]{DONE}

\item read about Spectacular AI \url{https://spectacular.ai/}\todo[color=green!30]{DONE}

\item read about DepthAI/OAKD\todo[color=green!30]{DONE}
\begin{itemize}
    \item \url{https://docs.luxonis.com/en/latest/pages/tutorials/first_steps/}
    \item  \url{https://shop.luxonis.com/collections/product-guide}
\end{itemize}

\item read about ROS2 \url{https://github.com/ros2/ros2}\todo[color=green!30]{DONE}


\end{itemize}


\subsection{Tasks}
\begin{itemize}
    \item Install Ubuntu 22\todo[color=green!30]{DONE}
    \item Install ROS2 Humble \url{http://docs.ros.org/en/humble/}\todo[color=green!30]{DONE}
    \item Basic ROS2 tutorials \url{http://docs.ros.org/en/humble/Tutorials.html}\todo[color=green!30]{DONE}
    \item Try out \url{https://turtlebot.github.io/turtlebot4-user-manual/software/turtlebot4_simulator.html}\todo[color=green!30]{DONE}
    \item Try out \url{https://turtlebot.github.io/turtlebot4-user-manual/software/rviz.html}\todo[color=green!30]{DONE}
    \item Try out \url{https://turtlebot.github.io/turtlebot4-user-manual/software/simulation.html}\todo[color=green!30]{DONE}
    \item Try out on the robot: \url{https://turtlebot.github.io/turtlebot4-user-manual/tutorials/}\todo[color=gray!30]{TODO}
\end{itemize}

\subsection{Achievements}
\begin{itemize}
    \item Read about Turtlebot4, ROSBot2, SPAI, DepthAI/OAK-D and ROS2
    \item Installed Ubuntu 22.04
    \item Installed ROS2 Humble
    \item Completed some basic ROS2 tutorials to get the hang of it
    \item Tried out the Turtlebot4 simulation model on Gazebo:\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/Gazebo_turtlebot4.png}
        \captionof{figure}{Turtlebot4 model in Gazebo}
    \end{minipage}

    \item Inspected the robot in RViz2 (Gazebo must be running to simulate the robot):\par
    \begin{minipage}{\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{images/rviz2_turtlebot4.png}
        \captionof{figure}{Turtlebot4 in RViz (simulated by Gazebo)}
    \end{minipage}
    
    \item Unfortunately I could not drive the simulated robot with the \verb|teleop_twist_keyboard| node
    
\end{itemize}

\newpage

\section{Nice tools}
\begin{itemize}
    \item Online ply viewer which works with big files: \url{https://point.love/}
    \item Online GSplat viewer (drag and drop a .ply file): \url{https://antimatter15.com/splat/}
    \item SuperSplat: \url{https://playcanvas.com/supersplat/editor}
\end{itemize}


\newpage


\iffalse
\section{EXAMPLE SECTION March 4 -- March 10}
\line(1,0){\linewidth}

\subsection{Current Hurdle/Problem}
\begin{itemize}
\item 
\end{itemize}
\subsection{Reading}
\begin{itemize}
\item 
\end{itemize}

\subsection{Programming}
\begin{itemize}
\item 
\end{itemize}
\subsection{Writing}
\begin{itemize}
\item 
\end{itemize}
\subsection{Insights Gained On the Current Problem}
\begin{itemize}
\item 
\end{itemize}
\subsection{Any Inspiration?}
\begin{itemize}
\item 
\end{itemize}
\subsection{Questions}
\begin{itemize}
\item 
\end{itemize}
\subsection{What's next?}
\begin{itemize}
\item
\end{itemize}
\newpage
\fi

\section{Highlighting Overview}

\colorbox{green}{Successfully completed}\\
\colorbox{yellow}{Pending}\\
\colorbox{orange}{Successfully completed, but didn't solve the problem/but has not effect on the process of the project.}\\
\colorbox{red}{Failed/Not Working}\\
\newpage



\bibliographystyle{plain}
\bibliography{main}

\end{document}
