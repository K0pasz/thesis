\chapter{Summary} \label{summary}

The goal of this thesis was to implement an environment mapping system on a mobile robot equipped with a stereo camera at Nokia Bell Labs Budapest. Additionally, the robot was intended to detect and interact with people by either following or avoiding them. Another objective was to establish a pipeline for generating photorealistic reconstructions of the mapped environment.

In the first semester of my Master of Science thesis, I became familiar with the Turtlebot4 mobile robot and its OAK-D stereo camera. I conducted experiments with the camera, the Spectacular AI SDK, photorealistic reconstructions, mapping, and object detection.

The Turtlebot4 offered considerable potential due to its advanced hardware: a Raspberry Pi 4 controller running Ubuntu 22.04 with ROS2 compatibility, and the OAK-D Pro stereo camera. The camera's capabilities included generating depth maps using its stereo lenses and accurately determining object positions using its VPU. Furthermore, with the combination of camera and IMU, the robot's poses can be calculated with higher frequency and more robustly than with each sensor alone. Leveraging the Spectacular AI visual-inertial odometry SDK, I created a mapping and data collection pipeline.

I implemented a ROS2 node for person detection, which published detected individuals' coordinates. This feature leveraged the camera's VPU to run neural networks for real-time object detection. However, a critical issue arose: once a person was detected, the IMU buffer overflowed, freezing the system. Even Luxonisâ€™ original object detection example code encountered the same problem, indicating a firmware-level issue. Consequently, I could not achieve robust person detection on the robot.

Photorealistic scene reconstruction is gaining traction, and as an additional goal, we sought to achieve reconstructions solely using data recorded during mapping. In the first semester, I tested NeRFs and Gaussian splatting with a handheld OAK-D camera, yielding promising results that motivated further exploration in the second semester.

I initially experimented with RTAB-Map for mapping due to its support for OAK-D cameras. Unfortunately, it proved unreliable, introducing lags that hindered practical use. Interestingly, RTAB-Map's iOS application performed smoothly on my phone.

%In the second semester, I explored NVIDIA's \verb|nvblox| and \verb|isaac-vslam| for mapping, implemented a custom mapping node using the Spectacular AI SDK, and developed a pipeline for creating photorealistic reconstructions.

Since RTAB-Map was unsuitable, I turned to \verb|nvblox|, a GPU-accelerated tool for voxel-based mapping compatible with the robot's camera. However, hardware limitations prevented full functionality; my GTX 1660 Ti GPU lacked the required 8 GB of VRAM, offering only 6 GB. I then explored \verb|isaac-vslam|, another NVIDIA tool for Visual SLAM that uses GPU acceleration. Its simulations can be run in NVIDIA Omniverse, which supports creating diverse environments. Unfortunately, Omniverse requires an RTX GPU for hardware-accelerated ray tracing, which my GTX GPU could not provide. Ultimately, deploying \verb|nvblox| or \verb|isaac-vslam| on the robot would require a Jetson with at least 8 GB of VRAM, but budget constraints precluded this option.

In the final phase of my thesis, I developed a ROS2 mapping node using the Spectacular AI SDK to compute the robot's poses and publish its current (time windowed) map as a point cloud. Additional nodes, running on a notebook collected poses, images, and point clouds at keyframes. After mapping, my image post-processing script removed blurry and overexposed images and their associated poses. Another script prepared the input data for Gaussian splatting by transforming robot poses and the point cloud into the required coordinate system and embedding camera intrinsics into a JSON file describing keyframe transformations. Using this dataset, I trained a Gaussian splatting model and generated splats. However, due to noise and misalignment in the point cloud, the results did not match my expectations for photorealistic reconstruction. To prove that this approach could be used well in environment mapping, I used COLMAP to estimate the poses of the keyframes and a point cloud of the scene. With this approach, I could achieve high-quality reconstructions of the environment.

Although the goals of the thesis were achieved in demonstrating the feasibility of robotic environment mapping using advanced reconstruction techniques, the results are not yet sufficient for practical applications. Further research is necessary to enable fully automated reconstruction of complex environments, such as home or industrial spaces, which remains the ultimate objective.

Several open problems were identified during this work. First, the Spectacular AI SDK currently supports only grayscale images, limiting the quality of photorealistic reconstruction. Second, the use of a fixed camera height creates challenges in capturing diverse perspectives and complete scene coverage. Addressing this may require an adjustable camera height or multiple cameras on the same robot, or even deploying multiple robots. However, using multiple robots introduces a new challenge: registering all images and data into a common coordinate system, which is an area requiring further investigation.

In the future, if the camera's manufacturer could solve the issues experienced in object detection, it would be possible to add a person detection feature to my mapping node. Another improvement would be a filtering and adjusting feature for the point cloud with which we would be able to create lifelike scenes from the mapped environment.

An especially promising direction for future work involves integrating photorealistic map representations, such as Gaussian splatting-based methods, directly into the SLAM process. Recent advances in Gaussian splatting-SLAM techniques~\cite{gsplat_slam}, which have gained attention in the recent years, demonstrate the potential for combining high-fidelity photorealistic reconstructions with traditional SLAM frameworks. However, these methods are currently computationally expensive, as they require training in real time; this is a challenge that remains unsolved. A potential approach to address this limitation would involve training only on small sections of the map incrementally, which could be feasible with a high-performance GPU (for example a Jetson), though not yet on typical robot hardware. This area represents an exciting avenue for research and development, with the potential to significantly advance the capabilities of robotic mapping systems.
