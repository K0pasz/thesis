\chapter{End-to-end mapping implementation} \label{implementation}


\section{Overview}

This chapter presents the implementation of my robotic mapping and image processing pipeline designed for a Turtlebot4 robot. Leveraging the capabilities of the Spectacular AI SDK and ROS2, I developed a custom ROS2 node that performs real-time mapping on the Turtlebot4. This node captures environmental data and processes it to generate keyframe-based mapping suitable for autonomous navigation and further spatial analysis.

In addition to the mapping node, I implemented secondary ROS2 nodes running on a laptop, responsible for saving images, their poses and the calculated point cloud captured at keyframe intervals. This process allows for high-resolution image and pose acquisition, preserving critical visual information for later post-processing. A post-process script subsequently cleans the image set by discarding blurry or overexposed images and their poses. 

This refined set of images, their poses and the point cloud is then used as input to Gaussian splatting to generate a smooth, visually coherent representation of the mapped environment. This pipeline supports high-quality spatial mapping with applications in various robotic and industrial domains, providing accurate visual models essential for analysis and further computational tasks.

\section{Mapping}

After my experiments with RTAB-Map and nvblox (see Chapter~\ref{experiments_3d_mapping}), it became clear that they are not suitable for my expectations. Due to RTAB-Map's lag and frequent lose of track and NVIDIA Isaac VSLAM's (and nvblox's) hardware requirements, I did not use them further in the final implementation of the mapping. On the other hand, the Spectacular AI SDK proved itself to be precise enough in my experiments (see Section~\ref{experiments_spai}) and we did not have to invest in additional hardware, so I decided to proceed with it. 

I created a ROS2 workspace for the robot which has a package that contains my \verb|spectacularai_node| (for more detail, see Appendix~\ref{spectacularai_node_code}). The node is the core component of my mapping implementation and is responsible for generating and publishing spatial data essential for real-time SLAM. The node, located within the \verb|spectacularai_depthai_turtlebot| package, is built with the Spectacular AI SDK.

The node initializes several ROS2 publishers that communicate mapping and image data. Key topics include \verb|/slam/odometry| for publishing the estimated pose, \verb|/tf| for broadcasting the transformation tree, \verb|/slam/left| for transmitting image data from the left camera, \verb|/slam/keyframe| for transmitting keyframe poses, \verb|/slam/pointcloud| for 3D point cloud data, and \verb|/slam/camera_info| to share camera intrinsic information.

Key methods in \verb|spectacularai_node| are as follows:
\begin{itemize}
    \item \verb|processOutput|: Monitors the VIO session output and processes each available frame.
    \item \verb|onVioOutput|: Called for each VIO output, publishing the odometry and TF data based on the calculated pose, which includes both position and orientation in the world frame.
    \item \verb|onMappingOutput|: This callback handles mapping outputs, iterating through updated keyframes and invoking methods to store and publish them.
    \item \verb|newKeyFrame|: Extracts pose and image data from keyframes, publishing each frame as a ROS2 message. The keyframe pose is converted to \textit{PoseStamped} format, while left-camera images are published via the \textit{CvBridge} (the ROS-OpenCV converter component).
    \item \verb|publishPointCloud|: Computes a 3D point cloud using position data transformed by the camera’s pose matrix. This point cloud is then published as a \textit{PointCloud2} message.
\end{itemize}

After launching the node on the robot, I got an error that the Spectacular AI Python SDK is not installed. I was not able to install it because the free version is only available for \textit{x86} CPU architecture, and the Raspberry Pi 4 is built with a \textit{aarch64} CPU architecture. After consulting with my advisor, he suggested that I should ask the Spectacular AI Team if they would grant me the SDK for my thesis, which they kindly did. I want to thank and give my best regards to the Spectacular AI Team and Jerry Ylilammi who gave me access to these SDK files. With the correct Python SDK installed, I was finally able to start the node successfully.

In addition to the robot’s workspace, a secondary ROS2 workspace was created specifically for the notebook located on the Turtlebot4’s platform. This workspace comprises 4 packages: \verb|rviz2_laptop|, \verb|keyframe_saver|, \verb|pointcloud_saver| and \verb|camera_info_saver|. The \verb|rviz2_laptop| package contains a launch file to initiate RViz, allowing for real-time visualization of data generated by the \verb|spectacularai_node|. This capability facilitates interactive monitoring and helps verify the mapping accuracy while the robot is in operation. The RViz visualized mapping of my mapping node can be seen on Figure~\ref{fig:spai_mapping_rviz}

\begin{figure}[htbp]
	\centering
	\includegraphics[width=150mm, keepaspectratio]{figures_jpg/spai_mapping_rviz.jpg}
	\caption{Mapping with Spectacular AI, visualized by RViz, point cloud shown from above}
	\label{fig:spai_mapping_rviz}
\end{figure}

\FloatBarrier
\section{Saving and processing data for Gaussian splatting}

The \verb|CameraInfoSaver| node in the \verb|camera_info_saver| package is a ROS2 utility designed to extract and store camera intrinsic parameters from \verb|CameraInfo| messages published on the \verb|/slam/camera_info| topic. This node plays a critical role in generating datasets for 3D reconstruction and SLAM applications by capturing essential camera calibration data required for accurate pose estimation and scene modeling in Gaussian splatting. This node extracts the image dimensions (height and width) and the intrinsic matrix (\textbf{K}), including focal lengths ($f_x$, $f_y$) and principal point ($c_x$, $c_y$). The node captures the distortion model and the complete intrinsic matrix as well. The extracted data is saved as a JSON file, ensuring compatibility with downstream processes that require camera calibration parameters.

The package called \verb|keyframe_saver| implements a ROS2 node that subscribes to two topics: \verb|/slam/left| and \verb|/slam/keyframe|. This node, titled \textit{KeyframeImageSaver}, is responsible for capturing and saving images and poses whenever a keyframe pose is detected. The node’s code leverages OpenCV, alongside ROS2's \verb|cv_bridge|, to convert incoming ROS \textit{Image} messages to OpenCV-compatible images. When a new keyframe pose is received on the \verb|/slam/keyframe| topic, the node retrieves the latest image from \verb|/slam/left|, converts it, and stores it in a designated directory with a sequential filename. When a keyframe message is received the node saves the latest pose into a JSON file into the same folder as the images, named sequentially.

The \verb|PointCloudSaver| node is another ROS2 component of my pipeline designed to capture and save 3D point cloud data from the \verb|/slam/pointcloud| topic. It subscribes to the \verb|/slam/pointcloud| topic to receive \verb|PointCloud2| messages containing 3D spatial data and, in addition, to the \verb|/slam/keyframe| topic to trigger saving the most recent point cloud when a keyframe pose is published. The point cloud data is saved in the PLY (Polygon File Format) file format~\cite{ply_format}, widely used for storing 3D models and point cloud data. The file is initialized with a header specifying the ASCII format, vertex structure, and placeholder for the vertex count. Subsequent point data is appended to the file, which assumes that the poses provided by VIO are accurate. If the pose estimation suffers from errors, such as drift or sudden jumps, the appended points may be placed in incorrect locations, resulting in an inconsistent global point cloud. The header's element vertex count is dynamically updated after each save operation to ensure file consistency.

I would like to note here that it would be safer to save per-frame point clouds individually, allowing offline pose optimization to refine the poses and create a more accurate global point cloud. Implementing this approach was beyond the scope of this thesis but is recommended as a direction for future work.

This saving functionality is essential to the mapping workflow, as the saved data serve as the basis for further processing. There are 2 main post-processing scripts that prepare the collected dataset for photorealistic reconstruction:
\begin{itemize}
    \item one for image processing,
    \item another one for pose and point transformation and dataset copying.
\end{itemize}

\subsection{Image processing}

The image post-processing script used in this thesis further refines the keyframe images captured by the \textit{KeyframeImageSaver} node. By automatically identifying and discarding low-quality images — namely, those that are either blurry or overexposed — this script ensures that only high-quality images are retained for the photogrammetry reconstruction process. After filtering, the script renames the remaining images and their poses in a sequential order, which aids in organizing the dataset for subsequent processing steps.

To determine if an image is blurry, the script employs the variance of the Laplacian method~\cite{blur_detection}. This method calculates the variance of pixel intensity in the Laplacian of the grayscale version of the image. The Laplacian operator is defined as the sum of second-order partial derivatives:

\begin{equation}
 \triangle \mathit{f}\left (x,y  \right )= \frac{\partial^2 f }{\partial x^2} + \frac{\partial^2 f }{\partial y^2}
\end{equation}

where \(f(x,y)\) represents the grayscale image intensity at pixel location \((x,y)\). Variance measures the spread of values around the mean; a high variance indicates sharpness (high contrast edges), while a low variance suggests blurriness (low contrast edges). In this script, I defined a threshold for variance: if the computed variance is below this threshold, the image is considered blurry and is flagged for deletion. 
The corresponding code snippet can be inspected here:

\FloatBarrier
\begin{lstlisting}[language=python,frame=single,float=!ht]
def is_blurry(image_path, threshold):
    """Check if an image is blurry using the variance of the Laplacian."""
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    if image is None:
        return False
    laplacian_var = cv2.Laplacian(image, cv2.CV_64F).var()
    return laplacian_var < threshold
\end{lstlisting}
\FloatBarrier

In this function, \verb|cv2.Laplacian| calculates the Laplacian of the grayscale image, and \verb|.var()| computes the variance of the resulting matrix. If \verb|laplacian_var| is less than the threshold, the image is deemed blurry. I adjusted the threshold by doing experiments with blurry images, it performed best with the value of 500.

For overexposure detection, the script evaluates the average brightness of an image. An overexposed image typically has a high overall intensity across the majority of pixels, indicating that many parts of the image are too bright. This approach converts the image to grayscale and calculates the mean pixel value. If the mean brightness exceeds a predefined threshold, the image is considered overexposed. It is possible to detect underexposed images with this method as well, in that case if the mean brightness is under the threshold then the image is underexposed.

The mathematical foundation here is based on calculating the mean of pixel intensities (\(I\)) in a grayscale image, which is computed as:
\begin{equation}
    \textbf{Mean Brightness} = \frac{1}{N}\sum_{i=1}^{N} I\left ( i \right )
\end{equation}
where \(N\) is the total number of pixels in the image. A high mean value signifies overexposure.

The code for overexposure detection is as follows:

\FloatBarrier
\begin{lstlisting}[language=python,frame=single,float=!ht]
def is_overexposed(image_path, brightness_threshold):
    """Check if an image is overexposed based on brightness levels."""
    image = cv2.imread(image_path)
    if image is None:
        return False
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    mean_brightness = np.mean(gray)
    return mean_brightness > brightness_threshold
\end{lstlisting}
\FloatBarrier
Here, \verb|np.mean(gray)| computes the average brightness, and if this value exceeds the \verb|brightness_threshold|, the image is flagged for deletion. I adjusted the threshold by doing experiments with overexposed images, it performed best with the value of 200, which is approximately 78\% of the grayscale range (255).

After the detection of overexposed and blurry images, the script iterates over all images in the specified directory, deletes any images (and their poses) identified as blurry or overexposed, and finally renames the remaining images (and their poses as well) in a sequential order. This ensures a clean, orderly dataset that is ready for the reconstruction pipeline.

By filtering out suboptimal images, this script plays a crucial role in optimizing the dataset, ensuring that the photogrammetry process has a higher chance of generating accurate 3D reconstructions. This refinement contributes to the overall quality and reliability of the map generated in the final stages of processing.

\subsection{Pose and point transformation}

The pose and point transformation script plays a vital role in converting data collected from the ROS2 pipeline into the format required by \verb|splatfacto| in Nerfstudio. This script processes the positional and orientational data, performs necessary coordinate transformations, and organizes the dataset structure.

The transformations are applied to align the data with the conventions used in Nerfstudio, which follows the OpenGL/Blender coordinate system~\cite{opengl_coordinate_system}, where \(+X\) is right, \(+Y\) is up and \(-Z\) is the look-at direction. The camera uses the ROS convention where \(+X\) is forward, \(+Y\) is left and \(+Z\) is the up direction. To achieve this transformation, two main transformation matrices are used:

\begin{itemize}
    \item Vision to robotics transformation matrix
    \item Vision to graphics transformation matrix
\end{itemize}

They are defined as follows:

\[
T_\text{vision\_to\_robotics} = \begin{bmatrix}
0 & 0 & 1 & 0 \\
-1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\]

\[
T_\text{vision\_to\_graphics} = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & -1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\]

I note that in our case the vision to graphics transformation matrix is the same as graphics to vision.


To transform the poses, the following equations has to be calculated:

\begin{equation} \label{pose_rob_to_vis}
    \bm{p}_{\text{vision}} = \bm{T}_{\text{vision-to-robotics}} \cdot \bm{p}_{\text{robotics}}
\end{equation}

\begin{equation} \label{pose_vis_to_graph}
    \bm{p}_{\text{graphics}} = \bm{T}_{\text{vision-to-graphics}} \cdot \bm{p}_{\text{vision}}
\end{equation}

where \(\bm{p}\) stands for pose.

The following equation transforms a single point:

\begin{equation} \label{point_rob_to_vis}
    \bm{p}' = \bm{T}_{\text{vision-to-robotics}} \cdot \bm{p}
\end{equation}

where \(\bm{p}\) stands for point.

The script processes positional and orientational data provided as JSON files for each keyframe. These values are converted into \(4\times4\) transformation matrices that align the data with the Nerfstudio graphics coordinate system.

The pipeline involves the following steps:
\begin{enumerate}
    \item Loading Camera Intrinsics: The script reads intrinsic parameters from \verb|camera_info.json|, including focal lengths, principal points and image dimensions, ensuring compatibility with Nerfstudio's input format.
    \item Folder Structure Creation: It creates the required folder structure and copies images and poses into designated directories.
    \item Pose and point Conversion: Using the transformation matrices, the poses and point cloud in robotics conventions are converted to Nerfstudio graphics conventions. This ensures correct alignment of the 3D models with the dataset.
    \item Dataset Creation: A \verb|transforms.json| file is generated, combining the transformed poses, intrinsic parameters, and file paths. This file includes:
    \begin{itemize}
        \item File paths to images.
        \item \(4\times4\) transformation matrices for each keyframe.
        \item Camera intrinsic parameters.
        \item Reference to the transformed point cloud file.
    \end{itemize}
\end{enumerate}

The key function for pose transformation, according to \ref{pose_rob_to_vis} and \ref{pose_vis_to_graph}, is outlined below:
\FloatBarrier \begin{lstlisting}[language=python,frame=single,float=!ht]
def pose_to_transform_matrix(position, orientation):
    """Convert position and orientation to a 4x4 transformation matrix."""
    translation = np.array([position["x"], position["y"], position["z"]])
    rotation = R.from_quat([orientation["x"], orientation["y"], orientation["z"], orientation["w"]])
    
    # Initial transformation matrix
    pose_robotics = np.eye(4)
    pose_robotics[:3, :3] = rotation.as_matrix()
    pose_robotics[:3, 3] = translation

    # Transforming the matrix
    pose_vision = T_vision_to_robotics @ pose_robotics
    pose_graphics = pose_vision @ T_vision_to_graphics

    return pose_graphics.tolist()
\end{lstlisting}

The script also transforms the points (vertexes) in the point cloud into the required coordinate system, according to \ref{point_rob_to_vis}:

\begin{lstlisting} [language=python,frame=single,float=!ht]
for idx, line in enumerate(vertex_data):
    x, y, z = map(float, line)
    vertex_coords = np.array([x, y, z, 1])
    transformed_point = T_vision_to_robotics @ vertex_coords
    f.write(f"{transformed_point[0]} {transformed_point[1]} {transformed_point[2]}\n")
\end{lstlisting}

\subsection{Gaussian splatting}

With the filtered images, the camera intrinsics and the transformed poses and point cloud I was ready for creating the Gaussian splat. The next step is to train the Gaussian splat model using the \verb|ns-train| command. The usage can be seen here:
\FloatBarrier
\begin{lstlisting}[language=bash,frame=single,float=!ht]
ns-train splatfacto --data gsplat_input/
\end{lstlisting}
\FloatBarrier

The command initiates the training of the Gaussian splat model on the processed data in the \verb|gsplat_input/| folder. After the training has started the process can be inspected via the viewer as seen on Figure~\ref{fig:training_nerf_karcag} in a previous chapter.

This step is a core component of the pipeline, wherein the neural model learns to reconstruct scenes using Gaussian splatting. The model leverages the processed images to produce a probabilistic representation of the 3D environment, ideal for generating photorealistic splats.

When the training is complete (a session can be viewed in a YouTube video at Appendix~\ref{yt_gsplat}), a created splat can be exported into a \verb|.ply| file with the following command:
\FloatBarrier
\begin{lstlisting}[language=bash,frame=single,float=!ht]
ns-export gaussian-splat --load-config outputs/gsplat_input/splatfacto/DATE/config.yml --output-dir exports/splat/\end{lstlisting}
\FloatBarrier
The saved file then can be opened with a Gaussian splat viewer and the created splat can be inspected, I will present examples in Chapter~\ref{evaluation}.

The codebase of the implementation can be observed in my Github repository at Appendix~\ref{github_repo}.
